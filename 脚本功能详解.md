# LangSplatV2 各阶段Python脚本功能详解

本文档详细说明每个阶段调用的Python脚本具体做了什么。

---

## 一、预处理阶段：`preprocess.py`

### 主要功能
为场景的所有图像生成2D语言特征，为后续训练提供ground truth。

### 具体流程

#### 1. **初始化模型** (第344-372行)
```python
# 加载OpenCLIP模型（ViT-B-16）
model = OpenCLIPNetwork(OpenCLIPNetworkConfig)
# 加载SAM（Segment Anything Model）
sam = sam_model_registry["vit_h"](checkpoint=sam_ckpt_path)
mask_generator = SamAutomaticMaskGenerator(...)
```

#### 2. **读取图像** (第374-400行)
- 从`images/`文件夹读取所有图像
- 如果图像分辨率>1080P，自动缩放到1080P
- 将图像转换为PyTorch张量格式

#### 3. **为每张图像生成语言特征** (第113-168行，`create`函数)

**步骤3.1: SAM分割** (第296-329行，`sam_encoder`函数)
- 使用SAM自动生成4种不同粒度的mask：
  - `default`: 默认粒度
  - `s`: 小粒度（small）
  - `m`: 中粒度（medium）
  - `l`: 大粒度（large）
- 对mask进行NMS（非极大值抑制）去重，去除重叠和低质量mask
- 为每个mask提取对应的图像区域

**步骤3.2: OpenCLIP编码** (第176-189行，`_embed_clip_sam_tiles`函数)
- 对每个mask区域：
  - 裁剪并填充到224x224
  - 使用OpenCLIP的ViT-B-16模型提取512维特征向量
  - 对特征进行L2归一化
- 为4种粒度分别生成特征

**步骤3.3: 保存特征** (第170-174行，`sava_numpy`函数)
- 保存两个文件：
  - `*_f.npy`: 语言特征向量 (shape: [N, 512]，N为mask数量)
  - `*_s.npy`: 分割映射图 (shape: [4, H, W]，4个粒度级别)

### 输出结果
- 在`<dataset_path>/language_features/`目录下，为每张图像生成：
  - `<image_name>_f.npy`: 语言特征数组
  - `<image_name>_s.npy`: 分割映射图

### 关键技术点
- **多粒度分割**: 使用4种不同粒度捕获不同尺度的语义区域
- **Mask NMS**: 去除冗余mask，保留高质量分割
- **特征归一化**: 确保特征在单位球面上，便于后续相似度计算

---

## 二、训练阶段：`train.py`

### 主要功能
训练3个level的全局语义码本和稀疏系数场，将2D语言特征映射到3D高斯点云。

### 具体流程

#### 1. **初始化** (第36-60行)
```python
# 加载预训练的RGB高斯点云模型
gaussians.restore(model_params, opt)
# 加载2D语言特征
features = load_2d_language_feature(dataset.lf_path, device)
# 初始化向量量化器（RVQ）
rvq = ResidualVectorQuantizationWithClustering(...)
# 拟合码本（K-means聚类）
rvq.fit_quantizers(features)
# 将码本复制到高斯模型
gaussians._language_feature_codebooks.data.copy_(codebooks)
```

#### 2. **训练循环** (第76-183行)

**每次迭代的步骤：**

**步骤2.1: 选择随机相机** (第100-103行)
```python
viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1))
```

**步骤2.2: 渲染** (第108-110行)
```python
render_pkg = render(viewpoint_cam, gaussians, pipe, background, opt)
# 输出包含：
# - render: RGB图像
# - language_feature_weight_map: 语言特征权重图
# - viewspace_points: 视空间点
# - visibility_filter: 可见性过滤
```

**步骤2.3: 计算损失** (第112-134行)

如果启用语言特征训练（`opt.include_feature`）：
```python
# 获取ground truth语言特征
gt_language_feature, language_feature_mask = viewpoint_cam.get_language_feature(...)
# 根据迭代次数选择layer（渐进式训练）
layer_idx = min(int(iteration / 10000 * layer_num), layer_num - 1)
# 计算预测的语言特征
language_feature = gaussians.compute_layer_feature_map(language_feature_weight_map, layer_idx)
# 计算余弦损失
cosloss = cos_loss(language_feature * mask, gt_language_feature * mask)
loss += cosloss
```

否则（仅RGB训练）：
```python
Ll1 = l1_loss(image, gt_image)
loss = (1.0 - lambda_dssim) * Ll1 + lambda_dssim * (1.0 - ssim(image, gt_image))
```

**步骤2.4: 反向传播和优化** (第134-176行)
```python
loss.backward()
# 梯度累积
if iteration % accum_iter == 0:
    gaussians.optimizer.step()
    gaussians.optimizer.zero_grad()
```

**步骤2.5: 保存checkpoint** (第178-182行)
```python
if iteration in checkpoint_iterations:
    torch.save((gaussians.capture(opt.include_feature), iteration), 
               scene.model_path + "/chkpnt" + str(iteration) + ".pth")
```

#### 3. **多层级训练**
- 脚本会为3个不同的`feature_level`（1, 2, 3）分别训练
- 每个level对应不同分辨率的特征表示
- 通过`train.sh`循环调用3次，每次设置不同的`--feature_level`

### 输出结果
- `output/<场景名>_<索引>_{1,2,3}/chkpnt<迭代次数>.pth`: 3个level的模型checkpoint
- 每个checkpoint包含：
  - 高斯点云参数（位置、颜色、不透明度等）
  - 语言特征码本（`_language_feature_codebooks`）
  - 稀疏系数场（`_language_feature_logits`）

### 关键技术点
- **向量量化（VQ）**: 使用Residual Vector Quantization压缩512维特征到64维码本
- **渐进式训练**: 根据迭代次数逐步增加使用的量化层数
- **Top-K检索**: 每个高斯点使用top-k个码本向量的加权组合

---

## 三、评估阶段：`eval_lerf.py`

### 主要功能
评估训练好的模型在文本查询任务上的性能（分割和定位）。

### 具体流程

#### 1. **加载ground truth** (第61-102行，`eval_gt_lerfdata`函数)
```python
# 读取LERF格式的标注文件（frame_xxxxx.json）
# 每个json包含：
# - bboxes: 边界框坐标
# - segmentation: 多边形分割标注
# - category: 类别标签（如"teapot", "book"）
# 转换为mask格式
```

#### 2. **评估循环** (第239-276行或第309-356行)

**两种模式：**
- **标准模式** (`evaluate`函数): 分别加载3个level的模型，分别渲染
- **快速模式** (`evaluate_quick`函数): 合并3个level的码本，一次性渲染

**步骤2.1: 加载模型** (第249-260行或第333-348行)
```python
# 加载3个level的checkpoint
for level_idx in range(3):
    gaussians.restore(model_params, args, mode='test')
    # 在快速模式下，合并codebooks和weights
```

**步骤2.2: 渲染语言特征图** (第262行或第350行)
```python
# 渲染当前视图的语言特征
language_feature_image = render_language_feature_map_quick(...)
# 输出: [3, 512, H, W] (3个level，512维特征)
```

**步骤2.3: 文本查询匹配** (第272行或第352行)
```python
# 设置查询文本
clip_model.set_positives(list(img_ann.keys()))  # 如["teapot", "book"]
# 计算相关性图
c_iou_list, c_lvl = segmentation_process_cuda(...)
```

#### 3. **分割处理** (第111-156行，`segmentation_process_cuda`函数)

**步骤3.1: 计算相关性** (第113行)
```python
valid_map = clip_model.get_max_across_quick(sem_map)
# 输出: [n_head, n_prompt, H, W]
# n_head=3 (3个level), n_prompt=查询数量
```

**步骤3.2: 平滑和归一化** (第123-133行)
```python
# 使用平均池化平滑
avg_pool = AvgPool2d(kernel_size=29, ...)
avg_filtered = avg_pool(valid_map)
# 归一化到[0,1]
output = (output - min) / (max - min)
```

**步骤3.3: 生成mask** (第135-136行)
```python
# 阈值分割
mask_pred = (output > thresh).type(torch.uint8)
# 平滑mask
mask_pred = smooth_cuda(mask_pred)
```

**步骤3.4: 计算IoU** (第138-144行)
```python
# 与ground truth mask计算IoU
intersection = torch.sum(torch.logical_and(mask_gt, mask_pred))
union = torch.sum(torch.logical_or(mask_gt, mask_pred))
iou = intersection / union
```

**步骤3.5: 选择最佳level** (第147-154行)
```python
# 为每个查询选择相关性最高的level
chosen_lvl = torch.argmax(score_lvl)
```

#### 4. **定位评估** (第158-200行，`localization_process_cuda`函数)
```python
# 找到相关性最高的像素位置
coord = torch.nonzero((avg_filtered[i] == score).type(torch.uint8))
# 检查是否在ground truth bbox内
if (coord in bbox):
    acc_num += 1
```

#### 5. **输出评估结果** (第278-289行)
```python
# 计算平均IoU
mean_iou_chosen = sum(chosen_iou_all) / len(chosen_iou_all)
# 计算定位准确率
acc = acc_num / total_bboxes
```

### 输出结果
- 日志文件：包含mIoU、定位准确率等指标
- 评估结果保存在`eval_result/<场景名>_<索引>/`目录

### 关键技术点
- **多层级融合**: 自动选择最佳level进行查询
- **平滑处理**: 使用平均池化减少噪声
- **阈值分割**: 将连续的相关性图转换为二值mask

---

## 四、可视化阶段：`visualize_lerf.py`

### 主要功能
可视化文本查询的结果，生成RGB图像、热力图、分割mask等。

### 具体流程

#### 1. **加载模型和场景** (第81-95行)
```python
# 加载场景
scene = Scene(dataset, combined_gaussians, shuffle=False)
views = scene.getTrainCameras()
view = views[view_idx]
# 加载checkpoint
combined_gaussians.restore(model_params, args, mode='test')
```

#### 2. **合并3个level的码本** (第98-120行)
```python
# 为每个level加载codebooks和weights
for level_idx in range(3):
    gaussians.restore(...)
    language_feature_codebooks.append(...)
    weights, indices = get_weights_and_indices(...)
    language_feature_weights.append(weights)
# 合并所有level
combined_gaussians._language_feature_codebooks = torch.stack(...)
```

#### 3. **渲染语言特征** (第122-126行)
```python
language_feature_image, rendered_rgb = render_language_feature_map_quick(...)
# language_feature_image: [3, 512, H, W]
# rendered_rgb: [3, H, W]
```

#### 4. **计算相关性热力图** (第128-136行)
```python
# 设置查询文本
clip_model.set_positives([query_text])
# 计算相关性
relev_map = clip_model.get_max_across_quick(language_feature_image)
# 选择最佳level
best_level = torch.argmax(relev_map.max(...))
relev_heatmap = relev_map[best_level, 0].cpu().numpy()
```

#### 5. **生成可视化结果** (第138-185行)

**5.1 RGB图像** (第142-148行)
```python
rendered_rgb_np = rendered_rgb.permute(1, 2, 0).cpu().numpy()
plt.imsave(rgb_path, rendered_rgb_np)
```

**5.2 热力图** (第150-160行)
```python
# 使用turbo colormap
colormap_options = ColormapOptions(colormap="turbo", ...)
colormap_saving(heatmap_tensor, colormap_options, heatmap_path)
```

**5.3 叠加图像** (第162-169行)
```python
# RGB + 热力图半透明叠加
overlay = rgb * (1 - alpha * heatmap) + heatmap_colored * (alpha * heatmap)
```

**5.4 分割mask** (第171-176行)
```python
# 阈值分割
threshold = 0.4
mask = (relev_heatmap > threshold).astype(np.uint8)
vis_mask_save(mask, mask_path)
```

**5.5 Mask叠加** (第178-185行)
```python
# RGB + 红色mask叠加
mask_rgb[mask == 1] = [1.0, 0.0, 0.0]  # 红色
mask_overlay = rgb * 0.7 + mask_rgb * 0.3
```

### 输出结果
在`visualize_result/<场景名>_<索引>/<查询文本>/view_<视图索引>/`目录下：
1. **rgb.png**: 渲染的RGB图像
2. **heatmap.png**: 相关性热力图（turbo colormap）
3. **overlay.png**: RGB与热力图的叠加
4. **mask.png**: 二值化分割mask
5. **mask_overlay.png**: RGB与mask的叠加

### 关键技术点
- **快速渲染**: 使用`render_language_feature_map_quick`合并3个level，加速渲染
- **自动选择最佳level**: 根据相关性自动选择表现最好的level
- **多种可视化方式**: 提供热力图、mask等多种可视化选项

---

## 五、整体流程总结

```
预处理 (preprocess.py)
  ↓
[输入] 原始图像
  ↓
[处理] SAM分割 → OpenCLIP编码 → 保存特征
  ↓
[输出] language_features/*.npy

训练 (train.py) × 3次（3个level）
  ↓
[输入] RGB高斯模型 + 语言特征
  ↓
[处理] 初始化码本 → 训练循环 → 优化参数
  ↓
[输出] output/*/chkpnt*.pth

评估 (eval_lerf.py)
  ↓
[输入] 训练好的模型 + 查询文本
  ↓
[处理] 渲染特征 → 文本匹配 → 计算IoU
  ↓
[输出] 评估指标（mIoU, 定位准确率）

可视化 (visualize_lerf.py)
  ↓
[输入] 训练好的模型 + 查询文本 + 视图索引
  ↓
[处理] 渲染特征 → 计算相关性 → 生成可视化
  ↓
[输出] RGB、热力图、mask等图像
```

---

## 六、关键技术细节

### 1. 向量量化（Vector Quantization）
- **目的**: 将512维的连续特征压缩到64维的离散码本
- **方法**: Residual Vector Quantization (RVQ)
- **优势**: 大幅减少存储和计算量，同时保持语义信息

### 2. 多层级特征
- **Level 1, 2, 3**: 对应不同分辨率的特征表示
- **训练**: 分别训练3个独立的模型
- **推理**: 自动选择或融合最佳level

### 3. Top-K检索
- 每个高斯点使用top-k个码本向量的加权组合
- 默认`topk=4`，平衡精度和效率

### 4. 快速渲染优化
- 合并3个level的码本，一次性渲染
- 使用einsum等高效操作
- 避免重复加载模型

### 5. 文本查询匹配
- 使用OpenCLIP编码查询文本
- 计算文本特征与场景特征的余弦相似度
- 通过阈值分割生成mask

---

## 七、常见参数说明

### preprocess.py
- `--dataset_path`: 数据集路径
- `--resolution`: 图像分辨率（-1表示自动）
- `--sam_ckpt_path`: SAM模型checkpoint路径

### train.py
- `-s`: 数据集路径
- `-m`: 模型输出路径
- `--start_checkpoint`: RGB预训练模型checkpoint
- `--feature_level`: 特征层级（1, 2, 3）
- `--vq_layer_num`: 向量量化层数（默认1）
- `--codebook_size`: 码本大小（默认64）
- `--topk`: Top-K检索数量（默认4）
- `--cos_loss`: 使用余弦损失

### eval_lerf.py
- `-s`: 数据集路径
- `-m`: 模型路径
- `--checkpoint`: checkpoint迭代次数
- `--mask_thresh`: mask阈值（默认0.4）
- `--topk`: Top-K值（默认4）
- `--quick_render`: 使用快速渲染模式

### visualize_lerf.py
- `-s`: 数据集路径
- `--dataset_name`: 数据集名称
- `--index`: 模型索引
- `--checkpoint`: checkpoint迭代次数
- `--query`: 单个查询文本
- `--queries`: 多个查询文本
- `--view_idx`: 视图索引
- `--topk`: Top-K值
